{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c3dcc4d",
   "metadata": {},
   "source": [
    "Loading Groq Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be171dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e6864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llamaChatModel = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98042b",
   "metadata": {},
   "source": [
    "#Prompts and Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ab9ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joseph P. Kennedy, the patriarch of the Kennedy family, had 28 grandchildren. He had nine children, and those children went on to have a total of 28 grandchildren.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an {profession} expert on {topic}\",\n",
    "        ),\n",
    "        (\"human\", \"Hello Mr {profession},can you please answer a question\"),\n",
    "        (\"ai\",\"Sure!\"),\n",
    "        (\"human\",\"{user_input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages=prompt.format_messages(\n",
    "    profession=\"Historian\",\n",
    "    topic=\"The kennedy family\",\n",
    "    user_input=\"How many grandchildren does Joseph P. Kennedy have ?\"\n",
    ")\n",
    "\n",
    "response=llamaChatModel.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929a9a5",
   "metadata": {},
   "source": [
    "Few shot examples with chaining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c3b9aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estoy bien, ¿y tú? (I'm fine, and you?)\n"
     ]
    }
   ],
   "source": [
    "# Creating a language transalation instructor\n",
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"¡hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"¡adiós!\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | llamaChatModel\n",
    "\n",
    "res = chain.invoke({\"input\": \"How are you?\"})\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08693dec",
   "metadata": {},
   "source": [
    "Output parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74fc370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "json_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Return a JSON object with an `answer` key that answers the following question: {question}\"\n",
    ")\n",
    "\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "\n",
    "json_chain = json_prompt | llamaChatModel | json_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c5a5a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res  = json_chain.invoke({\"question\": \"What is the biggest country?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d6af249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Russia'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c09e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
